name: Common Unit Tests

on:
  workflow_call:
    inputs:
      platform:
        required: true
        type: string
        description: Platform name (e.g., cuda, default)
      device:
        required: true
        type: string
        description: Device type (e.g., a100, a800, h100, generic)
      image:
        required: true
        type: string
      runs_on:
        required: true
        type: string
      container_volumes:
        required: true
        type: string
      container_options:
        required: true
        type: string
      ignored_tests:
        required: false
        type: string
        default: ''
        description: JSON array of test files to ignore (e.g., '["tests/unit_tests/test_a.py", "tests/unit_tests/test_b.py"]')

jobs:
  unit_test:
    defaults:
      run:
        shell: bash
    runs-on: ${{ fromJson(inputs.runs_on) }}
    strategy:
      fail-fast: false
      matrix:
        test_group:
          - name: core
            path: "tests/unit_tests/test_*.py"
            description: "Core unit tests"
          - name: transformer
            path: "tests/unit_tests/transformer/"
            description: "Transformer tests"
          - name: models
            path: "tests/unit_tests/models/"
            description: "Model tests"
          - name: distributed
            path: "tests/unit_tests/distributed/"
            description: "Distributed tests"
          - name: dist_checkpointing
            path: "tests/unit_tests/dist_checkpointing/"
            description: "Distributed checkpointing tests"
          - name: tensor_parallel
            path: "tests/unit_tests/tensor_parallel/"
            description: "Tensor parallel tests"
          - name: pipeline_parallel
            path: "tests/unit_tests/pipeline_parallel/"
            description: "Pipeline parallel tests"
          - name: inference
            path: "tests/unit_tests/inference/"
            description: "Inference tests"
          - name: data
            path: "tests/unit_tests/data/"
            description: "Data tests"
          - name: fusions
            path: "tests/unit_tests/fusions/"
            description: "Fusion tests"
          - name: ssm
            path: "tests/unit_tests/ssm/"
            description: "SSM/Mamba tests"
          - name: others
            path: "tests/unit_tests/a2a_overlap/ tests/unit_tests/export/ tests/unit_tests/post_training/ tests/unit_tests/tokenizers/ tests/unit_tests/utils/"
            description: "Other tests (a2a_overlap, export, post_training, tokenizers, utils)"
    name: unit-${{ inputs.device }}-${{ matrix.test_group.name }}
    container:
      image: ${{ inputs.image }}
      ports:
        - 80
      volumes: ${{ fromJson(inputs.container_volumes) }}
      options: ${{ inputs.container_options }}
    steps:
      - name: Checkout source code
        uses: actions/checkout@v6
        with:
          repository: ${{ github.event.pull_request.head.repo.full_name || github.repository }}
          ref: ${{ github.event.pull_request.head.ref || github.ref }}
          ssh-strict: true
          ssh-user: git
          ssh-key: ${{ secrets.RUNNER_SSH_KEY }}
          persist-credentials: false
          clean: true
          sparse-checkout-cone-mode: true
          fetch-tags: false
          show-progress: true
          lfs: false

      - name: Set safe directory
        run: |
          git config --global --add safe.directory $GITHUB_WORKSPACE

      - name: Setup Python environment
        working-directory: ${{ github.workspace }}
        run: |
          set -euo pipefail
          conda activate flagscale-train
          echo "Python location: $(which python)"
          echo "Python version: $(python --version)"
          # pip install pytest-mock nltk transformers==4.57.3 wandb tiktoken sentencepiece flask-restful
          # Install package in development mode
          pip install -e . --no-deps

          # Copy test data
          mkdir -p /opt/data
          wget -O /opt/data/datasets.zip https://baai-flagscale.ks3-cn-beijing.ksyuncs.com/temp/datasets.zip && unzip -o /opt/data/datasets.zip -d /opt/data && rm /opt/data/datasets.zip
          wget -O /opt/data/tokenizers.zip https://baai-flagscale.ks3-cn-beijing.ksyuncs.com/temp/tokenizers.zip && unzip -o /opt/data/tokenizers.zip -d /opt/data && rm /opt/data/tokenizers.zip
        timeout-minutes: 30

      - name: Run unit tests - ${{ matrix.test_group.name }}
        id: unit_test
        working-directory: ${{ github.workspace }}
        run: |
          set -euo pipefail

          PLATFORM='${{ inputs.platform }}'
          DEVICE='${{ inputs.device }}'
          TEST_GROUP='${{ matrix.test_group.name }}'
          TEST_PATH='${{ matrix.test_group.path }}'

          echo "Running unit tests: $TEST_GROUP"
          echo "Test path: $TEST_PATH"
          echo "Platform: $PLATFORM"
          echo "Device: $DEVICE"
          echo "Working directory: $GITHUB_WORKSPACE"

          # Display Python environment info
          echo "Python location: $(which python)"
          echo "Python version: $(python --version)"
          conda activate flagscale-train

          # Set environment variables
          export PYTHONPATH=$GITHUB_WORKSPACE:${PYTHONPATH:-}
          export TORCHINDUCTOR_CACHE_DIR=/tmp/.torch_inductor_cache
          mkdir -p $TORCHINDUCTOR_CACHE_DIR

          # Build ignore options from ignored_tests input
          IGNORED_TESTS='${{ inputs.ignored_tests }}'
          IGNORE_OPTS=""
          if [ -n "$IGNORED_TESTS" ] && [ "$IGNORED_TESTS" != "[]" ]; then
            echo "Parsing ignored tests list..."
            # Parse JSON array and build --ignore options
            IGNORE_OPTS=$(echo "$IGNORED_TESTS" | python3 -c "import sys, json; tests = json.load(sys.stdin); print(' '.join(['--ignore=' + t for t in tests])) if tests else None")
            if [ -n "$IGNORE_OPTS" ]; then
              echo "Ignoring tests: $IGNORE_OPTS"
            fi
          fi

          # Run unit tests with torchrun
          # Each test group runs in isolation to avoid state pollution
          torchrun --nproc_per_node=8 -m pytest -v $TEST_PATH $IGNORE_OPTS
          exit_code=$?

          if [ $exit_code -eq 0 ]; then
            echo "✅ Unit tests passed for $TEST_GROUP on $PLATFORM/$DEVICE"
          else
            echo "❌ Unit tests failed for $TEST_GROUP on $PLATFORM/$DEVICE (exit code: $exit_code)"
          fi

          echo "exit_code=$exit_code" >> $GITHUB_OUTPUT
          exit $exit_code
        timeout-minutes: 60
